{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 2: ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "from scipy import signal\n",
    "from scipy.linalg import fractional_matrix_power as matrix_power\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ICA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider ICA model:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{A}\\mathbf{S}$$\n",
    "\n",
    "where $\\mathbf{S} \\in \\mathbb{R}^n$ is the source signal, $\\mathbf{X} \\in \\mathbb{R}^m$ - observed data, $\\mathbf{A}$ - $m \\times n$ mixing matrix, where $m \\geq n, \\mathrm{Rank}(\\mathbf{A}) = n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)\n",
    "S = np.random.logistic(0, 1, (2, 2000)) ** 3\n",
    "S = S / S.std()\n",
    "S[0, :] /= 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_1$\")\n",
    "plt.hist(S[0,:], bins=50, density=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_2$\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.hist(S[1,:], bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Set mixing matrix $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 2 \\end{pmatrix}$ and define X as linear mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mixing matrix A\n",
    "A = np.array([[1, 1], [0, 2]])\n",
    "\n",
    "# set X as linear mixture of S\n",
    "X = np.dot(A, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X.T)\n",
    "ica = FastICA(n_components=2).fit(X.T)\n",
    "print(\"PCA components:\\n\", pca.components_)\n",
    "print(\"ICA components:\\n\", ica.mixing_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_mixing_0 = ica.mixing_[:,0] / np.linalg.norm(ica.mixing_[:,0])\n",
    "ica_mixing_1 = ica.mixing_[:,1] / np.linalg.norm(ica.mixing_[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"True data\")\n",
    "plt.scatter(S.T[:,0], S.T[:,1], alpha=0.1)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Mixed data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(X.T[:,0], X.T[:,1], alpha=0.1)\n",
    "\n",
    "plt.quiver(0, 0, ica_mixing_0[0], ica_mixing_0[1], zorder=11, width=0.01, scale=5, color='r', alpha=0.8, label='ICA')\n",
    "plt.quiver(0, 0, ica_mixing_1[0], ica_mixing_1[1], zorder=11, width=0.01, scale=5, color='r', alpha=0.8)\n",
    "\n",
    "plt.quiver(0, 0, pca.components_[0,0], pca.components_[1,0], zorder=11, width=0.01, scale=5, color='orange', alpha=0.8, label='PCA')\n",
    "plt.quiver(0, 0, pca.components_[0,1], pca.components_[1,1], zorder=11, width=0.01, scale=5, color='orange', alpha=0.8)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data whitening\n",
    "\n",
    "Consider a generated dataset which is sampled from multivariate normal distribution with covariance matrix $C = \\begin{pmatrix} 5 & 3 \\\\ 3 & 2 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size\n",
    "sample_size = 1000\n",
    "\n",
    "# sample from multivariate Gaussian\n",
    "mu = np.random.normal(0, 0.5, 2)\n",
    "C = np.array([[5, 3], [3, 2]])\n",
    "data = np.random.multivariate_normal(mu, C, size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = (data[:,0] > 3) & (data[:,1] > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.title(\"Original data, mu={:.2f}, {:.2f}, var=1, 1\".format(mu[0], mu[1]))\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.1)\n",
    "plt.scatter(data[idx,0], data[idx,1], alpha=0.5)\n",
    "plt.scatter(mu[0], mu[1], alpha=1, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitening has two simple steps:\n",
    "\n",
    "1. Project the dataset onto the eigenvectors. This rotates the dataset so that there is no correlation between the components.\n",
    "\n",
    "2. Normalize the the dataset to have a variance of 1 for all components. This is done by simply dividing each component by the square root of its eigenvalue.\n",
    "\n",
    "Let $\\mathbf{S} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T$ is eigenvalue decomposition of sample covariance matrix $\\mathbf{S} \\triangleq \\mathbf{X}^T\\mathbf{X}$.\n",
    "\n",
    "Then whitening matrix $\\mathbf{W}$ is defined as:\n",
    "\n",
    "$$\\mathbf{W} = \\mathbf{V} \\mathbf{\\Lambda}^{-1/2} \\mathbf{V^T}$$\n",
    "\n",
    "or alternatively (PCA variant): \n",
    "\n",
    "$$\\mathbf{W} = \\mathbf{V} \\mathbf{\\Lambda}^{-1/2}$$\n",
    "\n",
    "Whitening transform is defined as:\n",
    "\n",
    "$$\\mathbf{X}' = \\mathbf{W} \\mathbf{X}$$\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Implement whitening matrix and whitening transform. Use either `scipy.linalg.fractional_matrix_power` or properties of diagonal matrix to raise it into power of $-1/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample mean\n",
    "x_mean = np.mean(data, axis=0)\n",
    "\n",
    "# center data\n",
    "data_centered = data - x_mean\n",
    "\n",
    "# compute covariance matrix\n",
    "covariance = data_centered.T.dot(data_centered) / sample_size\n",
    "print(\"Covariance matrix:\\n\", covariance)\n",
    "\n",
    "# obtain eigenvectors of covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "# compute whitening matrix\n",
    "W_zca = eigenvectors.dot(matrix_power(np.diag(eigenvalues), -0.5)).dot(eigenvectors.T)\n",
    "print(\"ZCA Whitening matrix:\\n\", W_zca)\n",
    "\n",
    "# compute PCA whitening matrix\n",
    "W = eigenvectors.dot(matrix_power(np.diag(eigenvalues), -0.5))\n",
    "print(\"PCA whitening matrix:\\n\", W)\n",
    "\n",
    "# compute whitened data\n",
    "data_scaled = StandardScaler().fit_transform(data) #np.dot(data_centered, W)\n",
    "data_whitened = np.dot(data_centered, W)\n",
    "data_whitened_zca = np.dot(data_centered, W_zca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.suptitle(\"Scaling and whitening\", fontsize=13)\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Original data\")\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.2)\n",
    "plt.scatter(data[idx,0], data[idx,1], alpha=0.5)\n",
    "plt.scatter(mu[0], mu[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Scaled data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(data_scaled[:,0], data_scaled[:,1], alpha=0.2)\n",
    "plt.scatter(data_scaled[idx,0], data_scaled[idx,1], alpha=0.5)\n",
    "plt.scatter(np.mean(data_scaled, axis=0)[0], np.mean(data_scaled, axis=0)[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"PCA whitened\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(data_whitened[:,0], data_whitened[:,1], alpha=0.2)\n",
    "plt.scatter(data_whitened[idx,0], data_whitened[idx,1], alpha=0.5)\n",
    "plt.scatter(np.mean(data_whitened, axis=0)[0], np.mean(data_whitened, axis=0)[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"ZCA whitened\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(data_whitened_zca[:,0], data_whitened_zca[:,1], alpha=0.2)\n",
    "plt.scatter(data_whitened_zca[idx,0], data_whitened_zca[idx,1], alpha=0.5)\n",
    "plt.scatter(np.mean(data_whitened_zca, axis=0)[0], np.mean(data_whitened_zca, axis=0)[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplots_adjust(top = 0.82)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ICA solution\n",
    "\n",
    "### Kurtosis approach - make the output signal components most non-Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA algorithm is conceptually relatively simple and works as follows:\n",
    "\n",
    "1. remove the mean and whiten the data\n",
    "2. rotate the data such that output signal components are most non-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixture of two sources: $X_1 \\sim Laplace(0,1)$ and $X_2 \\sim U(0,1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample size and dimensionality\n",
    "n, d = 5000, 2\n",
    "\n",
    "X_1 = np.random.uniform(-np.sqrt(3),np.sqrt(3), (n))\n",
    "X_2 = np.random.laplace(0, 1, (n))\n",
    "X = np.vstack([X_1, X_2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_2 \\sim U(0,1)$\")\n",
    "plt.hist(X_1, bins=50, density=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_1 \\sim Laplace(0,1)$\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.hist(X_2, bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining random mean and random mixing matrix $M$  obtain mixed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set random mean\n",
    "mean = np.random.normal(0, 0.5, d)\n",
    "\n",
    "# create random mixing matrix\n",
    "M = np.random.normal(0, 1, (d, d))\n",
    "\n",
    "# mix initial sources and add mean.\n",
    "Y = np.dot(M, X) + mean.reshape((2, 1))\n",
    "\n",
    "print(\"\\nInitial sources:\\n\", X)\n",
    "print(\"\\nInitial mean:\\n\", mean)\n",
    "print(\"\\nMixing matrix:\\n\", M)\n",
    "print(\"\\nMixed data:\\n\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Mixed data\")\n",
    "plt.scatter(Y[0], Y[1], alpha=0.2)\n",
    "plt.scatter(np.mean(Y.T, axis=0)[0], np.mean(Y.T, axis=0)[1], alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white(data):\n",
    "    x_mean = np.mean(data, axis=1)\n",
    "    data_centered = data - x_mean.reshape((2, 1))\n",
    "    covariance = data_centered.dot(data_centered.T) / n\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "    W = eigenvectors.dot(matrix_power(np.diag(eigenvalues), -0.5)).dot(eigenvectors.T)\n",
    "    return np.dot(W, data_centered), data_centered, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** whiten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_white, Y_centered, W = white(Y)\n",
    "Y_white.shape, Y_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Whiten data\")\n",
    "plt.scatter(Y_white[0], Y_white[1], alpha=0.2)\n",
    "plt.scatter(np.mean(Y_white, axis=1)[0], np.mean(Y_white, axis=1)[1], alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Define function for calculating fourth order autocumulants for zero mean data.\n",
    "\n",
    "The loss (contrast) function is based on fourth order auto cumulants, which are calculated with the following function.\n",
    "\n",
    "Four-order cumulant (corrected moment) is defined:\n",
    "\n",
    "$$C_{iiii} = M_{iiii} - 3 M_{ii}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourthOrderAutoCumulants(data_centered):\n",
    "    \n",
    "    moment2ndOrder = np.average(data_centered ** 2, axis=1)\n",
    "    moment4thOrder = np.average(data_centered ** 4, axis=1)\n",
    "    cumulants4thOrder = moment4thOrder - 3 * moment2ndOrder ** 2\n",
    "    \n",
    "    return cumulants4thOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fourthOrderAutoCumulants(Y_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define contrast function for whitened data.\n",
    "\n",
    "The contrast function is a scalar function that quantifies on the basis of fourth order cumulants how non-Gaussian the components of a signal are:\n",
    "\n",
    "$$\\Psi := \\sum_i C_{iiii}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastFunction(whitenedData):\n",
    "    cumulants = fourthOrderAutoCumulants(whitenedData)\n",
    "    contrast = np.sum(cumulants ** 2)\n",
    "    \n",
    "    return contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contrastFunction(Y_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "**Step 2:** Find optimal rotation angle with random Givens rotations.\n",
    "\n",
    "In higher dimensions a rotation matrix can be quite complex. To keep things simple, we use Givens rotations, which are defined as a rotation within the 2D subspace spanned by two axes n and m. For instance a  rotation matrix in 2D is given by:\n",
    "\n",
    "$$\\mathbf{R} = \\begin{pmatrix} cos \\phi & sin \\phi \\\\ -sin \\phi & cos \\phi \\end{pmatrix}$$\n",
    "\n",
    "A rotation within the plane spanned by the second and the fourth axis ($n$ = 2, $m$ = 4) of a four-dimensional space is given by\n",
    "\n",
    "$$\\mathbf{R} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0  & cos \\phi & 0 & sin \\phi \\\\ 0 & 0 & 1 & 0 \\\\ 0 & -sin \\phi & 0 & cos \\phi \\end{pmatrix}$$\n",
    "\n",
    "It can be shown that any general rotation, i.e. any orthogonal matrix with positive determinant, can be written as a product of Givens-rotations. Thus, we can find the general rotation matrix of the ICA problem by applying a series of Givens rotations and each time improve the objective function. By applying enough of such Givens-rotations, the algorithm should eventually converge to the globally optimal solution.\n",
    "\n",
    "This is a simple procedure to rotate the data such that the contrast function is maximized, i.e. such that the components are maximally non-Gaussian, which implies that the components are minimally statistically independent (as far as fourth order is concerned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with rotation angle 0, i.e. with rotation matrix [[1,0],[0,1]] and contrast of the whitened data.\n",
    "\n",
    "bestAngle = 0\n",
    "bestRotationMatrix = np.array([[1,0],[0,1]])\n",
    "bestRotatedContrast = contrastFunction(Y_white)\n",
    "\n",
    "angle = 0\n",
    "for iteration in np.arange(0, 30, 0.01):\n",
    "\n",
    "    # Try a new rotation angle near the old one.\n",
    "    angle = angle + np.random.normal(0,m.exp(-iteration))    # Vary the angle a bit, where 'a bit' gets exponentially smaller.\n",
    "    rotationMatrix = np.array([[m.cos(angle),-m.sin(angle)],[m.sin(angle),m.cos(angle)]]) # Calculate new rotation matrix.\n",
    "    rotatedData = np.dot(rotationMatrix, Y_white)        # Calculate rotated whitened data.\n",
    "    rotatedContrast = contrastFunction(rotatedData)          # Calculate contrast ov newly rotated whitened data.\n",
    "    \n",
    "    # If contrast has improved, keep the new rotation angle, matrix, and contrast, otherwise continue with the old ones.\n",
    "    if rotatedContrast > bestRotatedContrast:\n",
    "        bestAngle = angle\n",
    "        print(m.cos(angle), m.sin(angle))                     # Print the cos and sin of the improved angle to demonstrate progress (the angle is not so useful, since it has a 2pi periodicity, and the problem has a 90° symmetry)\n",
    "        bestRotationMatrix = rotationMatrix\n",
    "        bestRotatedContrast = rotatedContrast\n",
    "        \n",
    "# Display final result.\n",
    "print(\"Best angle: \", bestAngle)\n",
    "print(\"Best rotation matrix:\\n\", bestRotationMatrix)\n",
    "print(\"Best rotated contrast: \", bestRotatedContrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Find unmixing matrix $\\mathbf{U} = \\mathbf{R} \\mathbf{W}$ and separated sources $\\mathbf{S} = \\mathbf{U} \\mathbf{\\bar{Y}}$, where $\\mathbf{\\bar{Y}}$ is centered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unmixing matrix U\n",
    "U = np.dot(bestRotationMatrix, W)\n",
    "\n",
    "# find separated sources S\n",
    "S = np.dot(U, Y_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.xlabel(\"$S_1$\")\n",
    "plt.ylabel(\"$S_2$\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Separated data\")\n",
    "plt.scatter(S[0], S[1], alpha=0.2)\n",
    "plt.scatter(np.mean(S, axis=1)[0], np.mean(S, axis=1)[1], alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot the distribution of separated sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Separated 1\")\n",
    "plt.hist(S[0], bins=50, density=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Separated 2\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.hist(S[1], bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the unmixing matrix $\\textbf{U}$ is the inverse of the mixing matrix $\\textbf{M}$, and thus $\\textbf{U}\\textbf{M}=\\textbf{1}$.  However ICA can only be expected to work up to a permutation and different scaling of the sources, thus $\\textbf{U}\\textbf{M}$ should look like a permutation matrix with arbitrary (but significantly different from zero) numbers instead of the ones, i.e. each row and each column should contain exactly one entry significantly different from 0.  However, since in our case the true sources as well as the estimated sources are normalized to variance one, the non-zero elements should be close to $\\pm1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Formally check whether your unmixing matrix is correct. Show they are true up to permutation and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "np.dot(U, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Blind source separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image size\n",
    "shape = (512, 512)\n",
    "rows, cols = shape\n",
    "\n",
    "# load images\n",
    "img1 = np.load('./data/camera.npy').flatten()\n",
    "img2 = np.load('./data/astronaut.npy').flatten()\n",
    "img3 = np.load('./data/moon.npy').flatten()\n",
    "img4 = np.load('./data/noise.npy').flatten()\n",
    "\n",
    "# combine images\n",
    "S = np.c_[img1, img2, img3, img4].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random mixing matrix A\n",
    "A = np.random.uniform(0.2, 0.8, (4, 4))\n",
    "\n",
    "# mix data\n",
    "X = np.dot(A, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "f, ax = plt.subplots(1, X.shape[0])\n",
    "f.set_size_inches((16,4))\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    ax[i].imshow(X[i,:].reshape(rows, cols), cmap=plt.gray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Run ICA with different number of `n_components`. Compare results, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ICA\n",
    "ica = FastICA(n_components=3)\n",
    "img_ica = ica.fit_transform(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "f, ax = plt.subplots(1, img_ica.shape[1])\n",
    "f.set_size_inches((16,4))\n",
    "\n",
    "for i in range(img_ica.shape[1]):\n",
    "    ax[i].imshow(img_ica[:,i].reshape(shape), cmap=plt.gray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3.5 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(1.5 * np.pi * time) # sawtooteh\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "#S += np.array([0, 0, np.random.normal()])  # Add noise\n",
    "\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "#A = np.random.normal(0, 1, (3, 3))\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "# Compute ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "\n",
    "# We can `prove` that the ICA model applies by reverting the unmixing.\n",
    "#assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n",
    "\n",
    "# For comparison, compute PCA\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "models = [X, S, S_, H]\n",
    "names = ['Observations (mixed signal)',\n",
    "         'True Sources',\n",
    "         'ICA recovered signals',\n",
    "         'PCA recovered signals']\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Homework\n",
    "\n",
    "1. Implement one of the following optimization schemes for ICA:\n",
    " - gradient descent for kurtosis (see lecture slides, pp. 49-51) or\n",
    " - gradient descent for negentropy (see lecture slides, pp. 53-59).\n",
    "  \n",
    "2. Apply ICA using obtained optimization procedure to the mixture of $X_1 \\sim Laplace(0,1)$ and $X_2 \\sim U(0,1)$ used in section (3) and blind source separation problems for images (4.1) and time series (4.2). For each dataset plot loss function value as a function of optimization step. Plot scatter and distribution plots for (3) for 5 different optimization steps. Plot separated signals for (4.1) and (4.2) for 5 different optimization steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
