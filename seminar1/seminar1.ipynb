{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 1: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_wine, load_boston, fetch_olivetti_faces, fetch_mldata\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Consider a generated dataset which is sampled from multivariate normal distribution with covariance matrix $C = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "\n",
    "mu = np.zeros(2)\n",
    "C = np.array([[3,1],[1,2]])\n",
    "data = np.random.multivariate_normal(mu, C, size=sample_size)\n",
    "\n",
    "# find true PC components by covariance matrix diagonalization E = V * L * V^-1\n",
    "l, V = np.linalg.eig(C)\n",
    "l, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find PCs and projection using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate PC components from empirical data\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "eigenvectors = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components(data, model, scatter=True):\n",
    "    W_pca = model.components_\n",
    "    \n",
    "    if scatter:\n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "    \n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"c\")\n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"c\")\n",
    "\n",
    "    plt.axis('equal')\n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "# plot true principal components\n",
    "plt.plot(data[:,0], (V[0,0] / V[0,1]) * data[:,0], color=\"g\")\n",
    "plt.plot(data[:,0], (V[1,0] / V[1,1]) * data[:,0], color=\"g\")\n",
    "\n",
    "# plot estimated principal components\n",
    "plot_principal_components(data, pca, scatter=False)\n",
    "\n",
    "plt.title(\"Multivariate gaussian\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Find PCs and projection manually\n",
    "\n",
    "Estimator of covariance matrix of normally distributed variables:\n",
    "\n",
    "$$\\hat{S}_{\\mathcal{N}} = \\frac{1}{n} (\\mathbf{X} - \\mathbf{\\bar{x}})^T (\\mathbf{X} - \\mathbf{\\bar{x}})$$\n",
    "\n",
    "Use either eigendecomposition `np.linalg.eig`:\n",
    "$$\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}$$\n",
    "\n",
    "or SVD `np.linalg.eig`:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U} \\mathbf{S} \\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# center the data\n",
    "mean = \n",
    "data_centered = \n",
    "\n",
    "# estimate data covariance matrix, X.T * X\n",
    "covariance_hat = \n",
    "\n",
    "# find eigenvalues and eigenvectors of data covariance matrix\n",
    "eigenvalues, eigenvectors = \n",
    "\n",
    "print(\"\\nNumerical mean of centered data (should be zero):\\n\", mean)\n",
    "print(\"\\nEstimation of covariance matrix:\\n\", covariance_hat)\n",
    "print(\"\\nEigenvalues:\\n\", eigenvalues)\n",
    "print(\"\\nEigenvectors\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersice:\n",
    "\n",
    "Compute and plot the projection on eigenbasis:\n",
    "\n",
    "Let $\\mathbf{V}'$ be $D \\times d$ matrix whose columns contain the largest eigenvectors and let $\\mathbf{X}$ be the original data $n \\times D$ matrix whose columns contain the different observations. Then the projected data $\\mathbf{X}'$ is obtained as $\\mathbf{X} = \\mathbf{X} \\mathbf{V}'$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute projection\n",
    "proj = \n",
    "proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.title(\"Projected data\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Real data\n",
    "\n",
    "### 2.1 Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points = np.loadtxt('./data/ref_points.csv', delimiter=',') # X coordinates\n",
    "X_train = np.loadtxt('./data/airfoils.csv', delimiter=',') # 199 wings\n",
    "X_test = np.loadtxt('./data/test_afl.csv', delimiter=',') # 200th wing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.ylim(-0.23, 0.23)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "#plt.plot(ref_points, test_point, '-*', label = 'Original test')\n",
    "plt.plot(ref_points, X_train[0, :], '-*', label = 'Airfoil #1')\n",
    "plt.plot(ref_points, X_train[1, :], '-*', label = 'Airfoil #2')\n",
    "plt.plot(ref_points, X_train[2, :], '-*', label = 'Airfoil #3')\n",
    "#plt.plot(ref_points, test_point, label = 'Original')\n",
    "\n",
    "plt.title(\"Airfoils\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Airfoil #1', 'Airfoil #2', 'Airfoil #3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airfoils principal components\n",
    "\n",
    "#### Exercise  \n",
    "Plot eigenvectors for the 5 largest and 5 smallest eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "components = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    fig = plt.figure(figsize=(6,2.5))\n",
    "    plt.ylim(-0.3, 0.3)\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    plt.plot(ref_points, pca.components_[i, :], '-*')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Component #{}'.format(i+1)])\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(len(pca.components_) - 5, len(pca.components_)):\n",
    "    fig = plt.figure(figsize=(6,2.5))\n",
    "    plt.ylim(-0.3, 0.3)\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    plt.plot(ref_points, pca.components_[i, :], '-*')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Component #{}'.format(i+1)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Explained variance is a ratio of sum of squared eigenvalues left to the sum of all eigenvalues:\n",
    "\n",
    "$$EV =  = \\frac{\\lambda_1^2}{\\lambda_1^2 + \\dots + \\lambda_n^2} = \\frac{\\sum_{i=1}^{d} \\lambda_i^2}{\\sum_{j=1}^{D} \\lambda_j^2}$$\n",
    "\n",
    "Plot expained `pca.explained_variance_ratio_` and cumulative explained variance, choose sample dimensionality, state the rule used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = \n",
    "CEV = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EV/CEVs\n",
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(EV, \"o-\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Cumulative explained variance\")\n",
    "plt.axhline(linewidth=1, y=0.99, color='r')\n",
    "plt.axhline(linewidth=1, y=0.95, color='r')\n",
    "plt.axhline(linewidth=1, y=0.9, color='r')\n",
    "plt.axhline(linewidth=1, y=0.8, color='r')\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(CEV, \"o-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Describe original airfoil vector in eigenbasis. Try different number of basis eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "eigen_vectors = pca.components_\n",
    "\n",
    "sample_mean = X_train.mean(axis=0)\n",
    "X_test_pcs = np.dot(np.dot(X_test - sample_mean, eigen_vectors.T), eigen_vectors) + sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.ylim(-0.23, 0.23)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "#plt.plot(ref_points, test_point, '-*', label = 'Original test')\n",
    "plt.plot(ref_points, X_test, '-*', label = 'Airfoil #200')\n",
    "plt.plot(ref_points, X_test_pcs, '-*', label = 'Airfoil #200 in eigenspace')\n",
    "\n",
    "plt.title(\"Airfoils\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Airfoil #1', 'Airfoil #1 in eigenspace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Measure reconstruction loss of out-of-sample airfoil `X_test` with different number of principal components in terms of 2-norm, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "norms = []\n",
    "\n",
    "for n in range(X_train.shape[1]):\n",
    "    pca = PCA(n_components=(n+1))\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    eigen_vectors = pca.components_\n",
    "    \n",
    "    sample_mean = X_train.mean(axis=0)\n",
    "    X_test_pcs = np.dot(np.dot(X_test - sample_mean, eigen_vectors.T), eigen_vectors) + sample_mean\n",
    "    \n",
    "    norms.append(###) # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6.5))\n",
    "plt.title(\"Reconstruction loss\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(norms)\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.ylabel(\"2-norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "train_size = 60000\n",
    "\n",
    "X_train, X_test, y_train, y_train = \\\n",
    "mnist.data[:train_size,:], mnist.data[train_size:,:], \\\n",
    "mnist.target[:train_size], mnist.target[train_size:]\n",
    "\n",
    "print(\"Dataset summary:\\nSamples: {}, features: {}, classes: {}\"\n",
    "      .format(X_train.shape[0] + X_test.shape[0], X_train.shape[1], np.unique(y_train).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot expained and cumulative explained variance, choose sample dimensionality, state the rule used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = \n",
    "CEV = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EV/CEVs\n",
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(EV)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Cumulative explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.axhline(linewidth=1, y=0.99, color='r')\n",
    "plt.axhline(linewidth=1, y=0.95, color='r')\n",
    "plt.axhline(linewidth=1, y=0.9, color='r')\n",
    "plt.axhline(linewidth=1, y=0.8, color='r')\n",
    "plt.plot(CEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersice\n",
    "\n",
    "Choose one of the digits $0 \\dots 9$ and encode it in terms of different number of principal compoments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "eigenvectors = pca.components_\n",
    "\n",
    "EV = pca.explained_variance_ratio_\n",
    "CEV = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode vector in eigenbasis\n",
    "sample_mean = X_train.mean(axis=0)\n",
    "X_test_pcs = np.dot(np.dot(X_test - sample_mean, eigenvectors.T), eigenvectors) + sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot numbers\n",
    "shape = (28, 28)\n",
    "_, ax = plt.subplots(10, 10, figsize=(10, 10),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(X_test_pcs[i].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Measure reconstruction loss of chosen digit within test data `X_test` with different number of principal components in terms of mean 2-norm and PSNR metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psnr import psnr\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCs interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. US arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_arrests = np.loadtxt(\"./data/usarrests.csv\", delimiter=\",\", skiprows=1, usecols=(1,2,3,4))\n",
    "targets = list(np.genfromtxt('./data/usarrests.csv', delimiter=',', dtype='str', skip_header=True)[:,0])\n",
    "y_arrests = [ x.replace('\"', '') for x in targets ]\n",
    "features_arrest = list(np.genfromtxt('./data/usarrests.csv', delimiter=',', dtype='str', skip_header=False)[0,1:])\n",
    "features_arrest = [ x.replace('\"', '') for x in features_arrest ]\n",
    "#X_arrests, y_arrests, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Standartize data to zero mean and unit variance with `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize data to zero mean and unit variance\n",
    "X_arrests_std = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_arrests_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set eigenvectors to project data to\n",
    "pc1, pc2 = 0, 1\n",
    "eigenvector_x = pca.components_[pc1]\n",
    "eigenvector_y = pca.components_[pc2]\n",
    "\n",
    "# project data into PC space\n",
    "xs = pca.transform(X_arrests_std)[:,pc1]\n",
    "ys = pca.transform(X_arrests_std)[:,pc2]\n",
    "\n",
    "# visualize projections\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "for i in range(eigenvector_x.shape[0]):\n",
    "    plt.arrow(0, 0, eigenvector_x[i] * max(xs), eigenvector_y[i] * max(ys), color='r', width=0.0005)\n",
    "    plt.text(eigenvector_x[i] * max(xs) + 0.05, eigenvector_y[i] * max(ys), features_arrest[i], color='r')\n",
    "\n",
    "# circles project documents (ie rows from csv) as points onto PC axes\n",
    "for i in range(xs.shape[0]):\n",
    "    plt.plot(xs[i], ys[i], 'bo', alpha=0.25)\n",
    "    plt.text(xs[i] - 0.05, ys[i] + 0.1, y_arrests[i], color='b', alpha=0.25)\n",
    "\n",
    "plt.title(\"Biplot\")\n",
    "plt.xlabel(\"PC \" + str(pc1 + 1))\n",
    "plt.ylabel(\"PC \" + str(pc2 + 1))\n",
    "plt.xlim((-3.5, 3.5))\n",
    "plt.ylim((-3.5, 3.5))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "features_wine = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "# X_wine, y_wine, features_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine_std = StandardScaler().fit_transform(X_wine)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_wine_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set eigenvectors to project data to\n",
    "pc1, pc2 = 0, 1\n",
    "eigenvector_x = pca.components_[pc1]\n",
    "eigenvector_y = pca.components_[pc2]\n",
    "\n",
    "# project data into PC space\n",
    "xs = pca.transform(X_wine_std)[:,pc1]\n",
    "ys = pca.transform(X_wine_std)[:,pc2]\n",
    "\n",
    "# visualize projections\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "for i in range(eigenvector_x.shape[0]):\n",
    "    plt.arrow(0, 0, eigenvector_x[i] * max(xs), eigenvector_y[i] * max(ys), color='r', width=0.0005, alpha=0.5)\n",
    "    plt.text(eigenvector_x[i] * max(xs) + 0.05, eigenvector_y[i] * max(ys), features_wine[i], color='r')\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "# circles project documents (ie rows from csv) as points onto PC axes\n",
    "for i in range(xs.shape[0]):\n",
    "    plt.plot(xs[i], ys[i], 'o', color=colors[y_wine[i]], alpha=0.25)\n",
    "\n",
    "plt.title(\"Biplot\")\n",
    "plt.xlabel(\"PC \" + str(pc1 + 1))\n",
    "plt.ylabel(\"PC \" + str(pc2 + 1))\n",
    "plt.xlim((-4.5, 4.5))\n",
    "plt.ylim((-4.5, 4.5))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA for decision making\n",
    "\n",
    "- high-dimensional space is more dangerous to overfit complex models, than simple ones\n",
    "- PC are orthogonal, decorellated features\n",
    "    - may improve linear classifiers, if data multicolinearity problem\n",
    "    - for complex decision boundaries classifiers, they can be a problem in even lower dimensions\n",
    "- variable scaling, ensure your data is scaled to isotropic Gaussian $\\sim \\mathcal{N}(0, 1)$ or at least have similar scale (`MinMaxScaler`)\n",
    "- you can estimate first $q$ PCs only, using iterative optimization, instead full eigenvalue/SVD decoposition and then taking top PC corresponing to largest eigenvalues\n",
    "\n",
    "### 4.1. Eigenfaces\n",
    "\n",
    "Eigenfaces as principal compoments and logistic regression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Olivetti faces dataset\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=1)\n",
    "train_size = 350\n",
    "X_train, X_test, y_train, y_test = dataset.data[:train_size,:], dataset.data[train_size:,:], dataset.target[:train_size], dataset.target[train_size:]\n",
    "shape = (64, 64)\n",
    "print(\"Dataset summary:\\nSamples: {}, features: {}, classes: {}\".format(dataset.data.shape[0], dataset.data.shape[1], np.unique(dataset.target).shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces\n",
    "_, ax = plt.subplots(10, 10, figsize=(10, 10),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(X_train[i].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot first 64 eigenfaces, comment the difference between the first and the last ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 64 components\n",
    "_, ax = plt.subplots(8, 8, figsize=(10, 10),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(pca.components_[i].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Encode any face in test dataset in eigenbasis of different powers, `n_components`= [2, 4, 8 16, 32, 64, $\\dots$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "face_id = 10\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "eigen_vectors = pca.components_\n",
    "\n",
    "sample_mean = X_train.mean(axis=0)\n",
    "X_test_pcs = np.dot(np.dot(X_test[face_id] - sample_mean, eigen_vectors.T), eigen_vectors) + sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_, ax = plt.subplots(1, 2, figsize=(5, 5),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "ax[0].imshow(X_test[face_id].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")\n",
    "ax[1].imshow(X_test_pcs.reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Classify faces in original dimension with kNN, logistic regression and random forest, conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc_full_train = accuracy_score(clf.predict(X_train), y_train)\n",
    "acc_full_test = accuracy_score(clf.predict(X_test), y_test)\n",
    "\n",
    "acc_full_train, acc_full_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Perform classification in the space of reduced dimension of your choice, conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = \n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "clf = \n",
    "clf.fit(X_train_pca, y_train)\n",
    "accuracy_score(clf.predict(X_train_pca), y_train), accuracy_score(clf.predict(X_test_pca), y_test), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Perform classification for a whole range of dimensions, with a classifier of your choice.\n",
    "- State the maximum number of principal components for this space, conclude. \n",
    "- Plot the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_train = []\n",
    "acc_test = []\n",
    "\n",
    "for n in range():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Classifier performance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.axhline(linewidth=1, y=acc_full_test, color='r')\n",
    "plt.plot(acc_train, 'b')\n",
    "plt.plot(acc_test, 'g')\n",
    "plt.legend(['Best on test in original dimension', 'Train in reduced dimension', 'Test in reducted dimension'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Homework\n",
    "\n",
    "Sberbank housing dataset  \n",
    "https://www.kaggle.com/c/sberbank-russian-housing-market\n",
    "\n",
    "### 5.1. PCA + interpretation\n",
    "\n",
    "As a homework, check which number of variables explains 80%, 95% and 99,5% of variance. Draw and interpret biplots of first two principal components and top 10 features with highest influence (largest absolute eigenvector coefficient) to each first two principal components.\n",
    "\n",
    "### 5.2. PCA + regression\n",
    "\n",
    "Check regression performance of various classifiers on raw data and of reduced data of various dimensions.\n",
    "\n",
    "Do a feature selection, selecting top 10 performing features, with any technique you know, compare them with top 10 features influencing first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
