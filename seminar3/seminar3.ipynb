{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 3: Intrinsic Dimension Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from sklearn.datasets import load_digits, fetch_olivetti_faces\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximum Likelihood\n",
    "\n",
    "MLE is the average of estimators $\\hat{m}_k$ for several values of $k$ where $\\hat{m}_k$ in turn is the average of local maximum likelihood estimators $\\hat{m}_k(x_i)$, which estimate the dimension of the data around a sample point $x_i$ based on the distances between $x_i$ and its $k$ nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of 'Maximum Likelihood Estimation of Intrinsic Dimension' by Elizaveta Levina and Peter J. Bickel\n",
    " \n",
    "how to use\n",
    "----------\n",
    " \n",
    "The goal is to estimate intrinsic dimensionality of data, the estimation of dimensionality is scale dependent\n",
    "(depending on how much you zoom into the data distribution you can find different dimesionality), so they\n",
    "propose to average it over different scales, the interval of the scales [k1, k2] are the only parameters of the algorithm.\n",
    " \n",
    "This code also provides a way to repeat the estimation with bootstrapping to estimate uncertainty.\n",
    " \n",
    "Here is one example with swiss roll :\n",
    " \n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X, _ = make_swiss_roll(1000)\n",
    " \n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=500, # nb_iter for bootstrapping\n",
    "                             verbose=1, \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "# the shape of intdim_k_repeated is (nb_iter, size_of_interval) where \n",
    "# nb_iter is number of bootstrap iterations (here 500) and size_of_interval\n",
    "# is (k2 - k1 + 1).\n",
    " \n",
    "# Plotting the histogram of intrinsic dimensionality estimations repeated over\n",
    "# nb_iter experiments\n",
    "plt.hist(intdim_k_repeated.mean(axis=1))\n",
    " \n",
    "\"\"\"\n",
    "# from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    " \n",
    "def intrinsic_dim_sample_wise(X, k=5):\n",
    "    neighb = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    dist, ind = neighb.kneighbors(X)\n",
    "    dist = dist[:, 1:]\n",
    "    dist = dist[:, 0:k]\n",
    "    assert dist.shape == (X.shape[0], k)\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1])\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    " \n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
    "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
    "    intdim_k = []\n",
    "    for k in range(k1, k2 + 1):\n",
    "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    " \n",
    "def repeated(func, X, nb_iter=100, random_state=None, mode='bootstrap', **func_kw):\n",
    "    if random_state is None:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    " \n",
    "    iters = range(nb_iter) \n",
    "    for i in iters:\n",
    "        if mode == 'bootstrap':\n",
    "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
    "        elif mode == 'shuffle':\n",
    "            ind = np.arange(nb_examples)\n",
    "            rng.shuffle(ind)\n",
    "            Xr = X[ind]\n",
    "        elif mode == 'same':\n",
    "            Xr = X\n",
    "        else:\n",
    "            raise ValueError('unknown mode : {}'.format(mode))\n",
    "        results.append(func(Xr, **func_kw))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Noisy) helix curve is given by parametric equation:\n",
    "\n",
    "$$H(t) = \\big( \\sin(2 \\pi t), \\cos(2\\pi t), t\\big) + \\varepsilon \\\\\n",
    "\\varepsilon \\sim \\mathcal{N}(0, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helix(t, var=0.02):\n",
    "    # your code here\n",
    "    x = \n",
    "    y = \n",
    "    z = \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500 # n samples\n",
    "linspace = np.linspace(-1, 1, n)\n",
    "\n",
    "data_helix = helix(linspace)\n",
    "data_helix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5,6))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(data_helix[0], data_helix[1], data_helix[2], alpha=0.95, c=((data_helix[2]+1)*255), cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Estimate sample dimensionality using repeated function and given parameters. Plot histogram, print mean values and std. It could be comfortable to make histogram in a new cell. Useful functions: repeated, matplotlib.pyplot.hist=plt.hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 10\n",
    "\n",
    "sample = data_helix\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_helix.T, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Plot mean value and mean value +- std as a function of a sample size.\n",
    "2. Plot mean value and mean value +- std as a function of a log_10(noise_std)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Airfoils\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Estimate intrinsic dimension of airfoils sample and compare results with PCA ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_airfoils = np.loadtxt('../seminar1/data/airfoils.csv', delimiter=',')\n",
    "data_airfoils.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 20\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_airfoils, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 MNIST\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Estimate intrinsic dimension of digits sample and compare results with PCA ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mnist = load_digits().data\n",
    "data_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 5\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_mnist, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Olivetti faces\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Estimate intrinsic dimension of faces sample and compare results with PCA ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces = fetch_olivetti_faces(shuffle=True).data\n",
    "data_faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 5\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_faces, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local PCA\n",
    "\n",
    "Let $\\{x_1, x_2, \\dots, x_n \\} \\subset \\mathbb{R}^D$ be a data sample and $U(x_i)$ - a neighborhood of each point, excluding $x_i$ itself, with cardinality $k$. By applying PCA locally, i.e. computing eigenvalues $\\lambda_{i,1} \\geq \\lambda_{i,2} \\geq \\dots \\lambda_{i,D}$ of each neighborhood covariance matrix: $\\mathbf{S}_i = \\frac{1}{k-1}(\\mathbf{U}(x_i) - x_i) (\\mathbf{U}(x_i) - x_i)^T$ and then averaging over eigenvalues $\\lambda_j = n^{-1} \\sum_{i=1}^n \\lambda_{i,j}$, where $j = 1, 2, \\dots, D$ one can estimate intrinsic dimension $d$ by thresholding variance explained with averaged eigenvalues.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "1. Implement Local PCA method for intrinsic dimensionality estimation (lecture slides, pp. 28-32).  \n",
    "2. Apply the method to artificial and real data, compare results with global PCA and MLE methods for different number of nearest neighbors $k$, conclude.\n",
    "\n",
    "For neighborhood selection you may use `kneigbbors` method from scikit-learn's `NearestNeighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k of nearest neighbors to select\n",
    "k = 9\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Compute explained and cumulative explained variance. Use results to estimate intrinsic dimension of data samples.\n",
    "\n",
    "$$EV_i := \\frac{\\lambda_i}{\\lambda_1 + \\dots + \\lambda_D}$$\n",
    "$$CEV_i := \\frac{\\sum_{i=1}^{d} \\lambda_i}{\\sum_{j=1}^{D} \\lambda_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = \n",
    "CEV = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EV/CEVs\n",
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(EV, \"o-\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Cumulative explained variance\")\n",
    "plt.axhline(linewidth=1, y=0.99, color='r')\n",
    "plt.axhline(linewidth=1, y=0.95, color='r')\n",
    "plt.axhline(linewidth=1, y=0.9, color='r')\n",
    "plt.axhline(linewidth=1, y=0.8, color='r')\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(CEV, \"o-\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
